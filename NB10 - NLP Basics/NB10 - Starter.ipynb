{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31512bf",
   "metadata": {},
   "source": [
    "# Lab Work 10 : NLP Basics\n",
    "\n",
    "This notebook builds on the tenth lecture of Foundations of Machine Learning. We'll focus on some *traditionnal* NLP technics, meaning not using any *transformer* architectures.\n",
    "\n",
    "Important note: the steps shown here are not always the most efficient or the most \"industry-approved.\" Their main purpose is pedagogical. So don't panic if something looks suboptimalâ€”it's meant to be.\n",
    "\n",
    "If you have questions (theoretical or practical), don't hesitate to bug your lecturer.\n",
    "\n",
    "We will try to accurately predict if a tweet has been written by Donald Trump (until its account was banned) or by an AI. To build this dataset, we used a dataset that has collected several Donald Trump's tweet and we manually ask several models to wrote copies. More details on how the dataset was made in a separate notebook.\n",
    "Let's load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Donald_or_AI_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed05dd",
   "metadata": {},
   "source": [
    "Tweets can be a challenge for NLP techniques :\n",
    "* Only short snippets of text : we kept only tweets below 150 characters\n",
    "* Some Twitter/X specificity : the \"@\" character and the \"#\" can carry meanings\n",
    "* There can be some emojis in it\n",
    "\n",
    "We will focus first on some cleaning before diving in the modelling.\n",
    "\n",
    "## Data cleaning\n",
    "\n",
    "In order to know what to perform, we suggest looking at some tweets or fake tweets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "df[\"writer\"] = df[\"model\"].apply(lambda string: \"Original\" if pd.isna(string) else string)\n",
    "\n",
    "indexes = sample(range(df.shape[0]), k=10)\n",
    "for index in indexes:\n",
    "    tweet = df[\"content\"][index]\n",
    "    writer = df[\"writer\"][index]\n",
    "    print(f\"[{writer}] {tweet}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ef74d",
   "metadata": {},
   "source": [
    "Given several rolls, we can see some points to clean :\n",
    "* Some tweets ends with \"\\n\" caracter\n",
    "* Some tweets are all within double quotes\n",
    "* Some tweets starts with \". \" then a quote, it is unnecessary to keep\n",
    "* Some tweets have double spaces, it is unnecessary to keep\n",
    "\n",
    "Beside this format, we also note that the deepseek-r1 model wrote very long tweet. Let's display one :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52358f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df[\"model\"] == \"deepseek-r1:1.5b\", ][\"content\"].values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0c867",
   "metadata": {},
   "source": [
    "The Deepseek-R1 model is a **reasoning model** meaning he *thinks* before answering. The only part we need here is the part outside of the thinking process.\n",
    "\n",
    "**Task** : Given all the previous discussion, write a `clean_tweet` function. It will also lower all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c4f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c3e604f",
   "metadata": {},
   "source": [
    "Now that we have *cleaner* tweets to work on, we need to build features from it.\n",
    "\n",
    "## Exploration and feature engineering\n",
    "\n",
    "**Task** : Create the following columns, with only the first one being build on the raw tweets. The rest of them shall be derived from a cleaned version.\n",
    "* `uppercase_ratio` : the proportion of uppercase character in the whole text. We may use the `isupper` method for a string.\n",
    "* `character_count` : the number of character in the text\n",
    "* `word_count` : the number of words in the text\n",
    "* `avg_word_length` : the average length of words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf32327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7edfd4",
   "metadata": {},
   "source": [
    "We would like to see if the previous indicators we build might already help identifying AI. \n",
    "\n",
    "**Task** : Using seaborn's [`histplot`](https://seaborn.pydata.org/generated/seaborn.histplot.html) function with the appropriate parameters, explore the columns. We shall use the *hue* parameter with either the target column `generated` or the `writer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997bee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72135861",
   "metadata": {},
   "source": [
    "## First modelisation\n",
    "\n",
    "With not much work, can we already perform a classification ?\n",
    "\n",
    "**Task** : Train a [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) in a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). We shall use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to measure performance. Here, we'll use the [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) metric, so we will probably need the [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) function.\n",
    "Bonus: use a [`TunedThresholdClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TunedThresholdClassifierCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13c7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a317df96",
   "metadata": {},
   "source": [
    "Already quite good performance without using the text *directly*.\n",
    "\n",
    "## NLP modelisation\n",
    "\n",
    "But it should be better with it.\n",
    "\n",
    "**Task** : Still using a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class and the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function, now use the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class with english stopwords and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9c160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1e66c5",
   "metadata": {},
   "source": [
    "That is better ! But we could imagine stronger performance if we tuned a bit the vectorizer.\n",
    "\n",
    "**Task** : Using the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class, find better parameter for the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812019cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8ffd060",
   "metadata": {},
   "source": [
    "But we only used words this time, not the previous statistics we had. \n",
    "\n",
    "## Third modelisation : combining approach\n",
    "\n",
    "**Task** : Using the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer), rewrite the pipeline so that it uses both numeric and text features.\n",
    "\n",
    "As the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) expect 1D array, one need to use first [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer) to flatten the input using the `squeeze` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541cb14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b44bf6c",
   "metadata": {},
   "source": [
    "It is better ! Time to fit it, then use the test set and submit on the [Kaggle competition](https://www.kaggle.com/t/db2bae0e9d814baa96a0468f021cd3f2).\n",
    "\n",
    "**Task** : Rewrite your feature engineering pipeline and use it to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07e55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a42e3e5a",
   "metadata": {},
   "source": [
    "Now, it is up to you to make the performance better ! Here are some guidelines:\n",
    "1. Make the dataset *cleaner* : there are probably still some work to do\n",
    "2. Build better features : more useful statistics can be extracted\n",
    "3. Find the most suitable model : note that we didn't fine-tune it, only the vectorizer..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
