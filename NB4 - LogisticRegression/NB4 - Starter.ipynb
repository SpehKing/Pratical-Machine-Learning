{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e281ad2",
   "metadata": {},
   "source": [
    "# Lab work 2 : Logistic regression\n",
    "\n",
    "This notebook builds on the fourth lecture of Foundations of Machine Learning. We'll focus on the logistic regression model and how to handle class imbalance.\n",
    "\n",
    "Important note: the steps shown here are not always the most efficient or the most “industry-approved.” Their main purpose is pedagogical. So don't panic if something looks suboptimal—it's meant to be.\n",
    "\n",
    "If you have questions (theoretical or practical), don't hesitate to bug your lecturer.\n",
    "\n",
    "We will try to accurately predict if a star observation is actually a [pulsars](https://en.wikipedia.org/wiki/Pulsar) on a [dataset](https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate). Let's first load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NB4 - Pulsars.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f1d04",
   "metadata": {},
   "source": [
    "As we are working on a classification problem, one need first to measure how imbalanced are classes.\n",
    "\n",
    "**Task**: Measure class imbalance on the dataset by measuring how often the target is the interest modality (here 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5b497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc629e8",
   "metadata": {},
   "source": [
    "Therefore, a model with accuracy below 90% is a model performing worst than a model predicting \"it is not a pulsar\" everytime. Beyond the difficulty in measuring performance, there is also a problem in the splitting process :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "imbalance_rate_train = 100 * y_train.mean()\n",
    "print(f\"Interest class rate: {imbalance_rate_train:.2f}% (train)\")\n",
    "imbalance_rate_test = 100 * y_test.mean()\n",
    "print(f\"Interest class rate: {imbalance_rate_test:.2f}% (test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ba1e3",
   "metadata": {},
   "source": [
    "The [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function split randomly the dataset, therefore doesn't preserve the imbalance across splits.\n",
    "To overcome this, one can use the *stratify* parameter.\n",
    "\n",
    "**Task** : Modify the code above to use the *stratify* parameter and conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118ee48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "457c462d",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "As always, the bread and butter of data science : data preparation.\n",
    "\n",
    "**Task** : Inspect the dataset with the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method as every column are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e2333b",
   "metadata": {},
   "source": [
    "**Task** : Given the code below, using [`scatter_matrix`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) function, interpret its output for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68314e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae0d6f7",
   "metadata": {},
   "source": [
    "This figure gives a lot of insights, but it doesn't show the proper repartition of the two classes.\n",
    "\n",
    "**Task** : Using the function below, explore again the different relationship between the variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8748a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addbfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "900ba5ba",
   "metadata": {},
   "source": [
    "We clearly get that : Skewness = Excess_kurtosis ** 2. Therefore, we decide to remove the Excess_kurtosis variable from our work.\n",
    "Also, given the name of the column *Mean* and *Std*, we decide to compute the *Z-Score* variable defined as the ratio between the two.\n",
    "\n",
    "**Task** : Implement the changes detailled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159802f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd8936d",
   "metadata": {},
   "source": [
    "Now is time model !\n",
    "\n",
    "## Modelisation and pipeline\n",
    "\n",
    "Logistic regression learns its coefficient using variants of gradient descent. Therefore it requires a standardisation of its feature to stabilize the training.\n",
    "\n",
    "**Task** : Using the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) class, train a [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) after splitting with the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function carefully.\n",
    "Then, display performance metric such as accuracy, precision, recall and F1-Score. One can use the [`precision_recall_fscore_support`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7becd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9264671",
   "metadata": {},
   "source": [
    "The performance are already quite good, as expected ! We are performing two steps to train :\n",
    "1. Learn and then standardize the inputs\n",
    "2. Train\n",
    "\n",
    "We are also performing two steps to predict :\n",
    "1. Standardize the input, using the learning in the training\n",
    "2. Predict\n",
    "\n",
    "This two use case are very similar : the can be combined using a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) class. Its goal is to simplify the code, as follow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd379ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logistic\", LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true=y_test, y_pred=y_pred, average=\"binary\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f010f",
   "metadata": {},
   "source": [
    "We highly encourage to use the Pipeline class in order to prevent data leakage with the StandardScaler.\n",
    "\n",
    "## Choosing the right threshold\n",
    "\n",
    "We know that a logistic regression output natively a score interpreted as a probability. Then is it transformed into classes (0 / 1) using the threshold 0.5\n",
    "What if this wasn't the best threshold for the F1-Score ?\n",
    "\n",
    "**Task** : Compute for several thresholds the precision, recall and F1-Score value. Then make a plot highlighting where the best performance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6c4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df9dab00",
   "metadata": {},
   "source": [
    "We are going to dive deeper in some subject for the continuation of this notebook, so let's wrap useful code up.\n",
    "\n",
    "**Task** : Define a function `train_experiment` which will fit a model given a training dataset, then test this model against a test dataset and display the metrics curves we've just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9603ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc36fae",
   "metadata": {},
   "source": [
    "## Handle imbalance\n",
    "\n",
    "So far, apart from splitting sets and performance measure, it doesn't feel like class imbalance really bothers us as the problem seems to be *simple*. Yet this type of problem can necessitate the need for **resampling**. We are going to cover some of them.\n",
    "\n",
    "The first is **random under sampling**. The method is going to randomly drop observations from the majority class, until the desired ratio between majority and minority class.\n",
    "\n",
    "**Task** : Using the [`RandomUnderSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html) class, balance the training dataset and measure the number of row before and after. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec61120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053cfbf6",
   "metadata": {},
   "source": [
    "The second method is the opposite : we are going to duplicate observations from the minority class until the desired ratio. This is called **random over sampling**.\n",
    "\n",
    "**Task** : Using the [`RandomOverSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) class, balance the training dataset and measure the number of row before and after. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba931360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1366918f",
   "metadata": {},
   "source": [
    "One last method that belongs to the under-sampling category is [**SMOTE**](https://www.jair.org/index.php/jair/article/view/10302/24590) (Synthetic Minority Over-sampling Technique). The algorithm is going to **create** observations based on neighbors of the future new observation. For a more visual explanation, one can look at [these slides](https://github.com/theo-lq/Conferences/blob/main/M2%20IASD%20Exec%20-%20ML%20LCLF/Support%202024.pdf) (in french).\n",
    "\n",
    "**Task** : Using the [`SMOTE`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) class, balance the training dataset and measure the number of row before and after. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ee91b7",
   "metadata": {},
   "source": [
    "For our dataset and our work, balancing the dataset didn't work out in performance but change the model confidence and calibration.\n",
    "\n",
    "Another method, not relying on balancing the dataset is to take class imbalance into the training loss.\n",
    "\n",
    "**Task** : Using the *class_weight* parameter in the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class, fit a model and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1096909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b383ca",
   "metadata": {},
   "source": [
    "## Polynomial feature and hyperparameter tuning\n",
    "\n",
    "As we have seen in session 2, polynomial features can help linear model handle complex relationships. We shoud try them again.\n",
    "\n",
    "**Task** : Using the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) class, process the training set and transform the test set. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec38ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc283a68",
   "metadata": {},
   "source": [
    "It is a bit better ! There is still some work to do !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
