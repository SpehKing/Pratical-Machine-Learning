{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2969e43f",
   "metadata": {},
   "source": [
    "## Lab Work 7: Ensemble Methods\n",
    "\n",
    "This notebook builds on the same lecture of Foundations of Machine Learning. We'll focus on Ensemble Methods.\n",
    "\n",
    "Important note: the steps shown here are not always the most efficient or the most \"industry-approved.\" Their main purpose is pedagogical. So don't panic if something looks suboptimal—it's meant to be.\n",
    "\n",
    "If you have questions (theoretical or practical), don't hesitate to bug your lecturer.\n",
    "\n",
    "First the necessary imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9095592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries for data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn tools for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensemble models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d819e",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Decision tree for regression: visualization. Generate synthetic data using a noisy Sin function and try to fit it with a decision tree regressor.\n",
    "\n",
    "The criterion is reduce the variance\n",
    "$\\text{Var}_{\\text{split}} = \\frac{|S_L|}{|S|}\\text{Var}(S_L) + \\frac{|S_R|}{|S|}\\text{Var}(S_R)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774720b",
   "metadata": {},
   "source": [
    "### MAGIC Gamma Telescope Dataset (UCI)\n",
    "\n",
    "This dataset comes from the MAGIC (Major Atmospheric Gamma Imaging Cherenkov) Telescope, used to study high-energy gamma rays. The objective is to classify whether each recorded event is: g → a gamma-ray event (signal), h → a hadronic shower (background noise).\n",
    "\n",
    "Import the dataset from the UCI Machine Learning repository using `fetch_ucirepo` with id=159 and split features from target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bd8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d806f16",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Build a preprocessing + Logistic Regression pipeline that will be the benchmark.\n",
    "Since all features are numerical, just pass them through with a ColumnTransformer, then add a LogisticRegression classifier to the pipeline. Fill in the code below to complete the baseline model setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b26ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77d018e1",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "The fundamental \"trinity\": fit your pipeline on the training set, generate predictions on the test set, and report performance metrics such as accuracy and the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cc5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3097db48",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Using `DecisionTreeClassifier` as introduced during the theoretical lecture, plot the performances on test and training in function of the depth as you did for the IRIS dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8afb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c5ac01",
   "metadata": {},
   "source": [
    "### Step 4: Bagging\n",
    "\n",
    "Compare Bagging and Random Forests using decision trees as base learners. Both are ensemble methods, but they differ in how diversity is introduced among the trees:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "   - Each tree is trained on a different bootstrap sample of the training data.\n",
    "   - All features are considered when splitting nodes.\n",
    "   - Final prediction is obtained by averaging (regression) or majority vote (classification).\n",
    "   - Reduces variance compared to a single decision tree, but may still overfit if trees are deep.\n",
    "\n",
    "Note: Bagging can be used with other base classifiers!\n",
    "\n",
    "2. Random Forest. Builds on bagging, but adds feature randomness:\n",
    "   - At each split, only a random subset of features is considered.\n",
    "   - This additional randomness further decorrelates trees, usually improving generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ce759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ccd37e",
   "metadata": {},
   "source": [
    "### Step 5: Boosting\n",
    "\n",
    "Compare boosting methods using shallow decision trees as base learners. Boosting builds an ensemble sequentially, where each model focuses on improving the errors of the previous ones.\n",
    "\n",
    "1. AdaBoost\n",
    "\n",
    "   - Sequentially fits weak learners, giving more weight to misclassified samples.\n",
    "   - Reduces bias of weak learners, but can be sensitive to noise.\n",
    "   - Final prediction: Each weak learner votes, weighted by its accuracy. The class with the highest total vote is predicted.\n",
    "\n",
    "2. Gradient Boosting\n",
    "\n",
    "   - Sequentially fits models to residual errors of previous models using gradient descent.\n",
    "   - More flexible than AdaBoost: supports regression, classification, and custom loss functions.\n",
    "   - t each step j, we compute the pseudo-residuals as the negative gradient of the loss with respect to the current model $F_{j-1}(x)$: $r_i^{(j)} = - \\left. \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right|_{F=F_{j-1}}$\n",
    "\n",
    "3. XGBoost (no details)\n",
    "   - Optimized, regularized version of gradient boosting.\n",
    "   - Faster training via parallelization and handles missing values efficiently.\n",
    "   - Often performs very well on large tabular datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb8bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
