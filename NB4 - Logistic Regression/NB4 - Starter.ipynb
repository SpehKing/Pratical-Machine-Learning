{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e281ad2",
   "metadata": {},
   "source": [
    "# Lab work 2 : Logistic regression\n",
    "\n",
    "This notebook builds on the fourth lecture of Foundations of Machine Learning. We'll focus on the logistic regression model and how to deal with class imbalance.\n",
    "\n",
    "Important note: the steps shown here are not always the most efficient or the most \"industry-approved.\" Their main purpose is pedagogical. So don't panic if something looks suboptimal—it's meant to be.\n",
    "\n",
    "If you have questions (theoretical or practical), don't hesitate to bug your lecturer.\n",
    "\n",
    "We will try to accurately predict if a star observation is actually a [pulsars](https://en.wikipedia.org/wiki/Pulsar) on a [dataset](https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate). Let's first load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NB4 - Pulsars.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f1d04",
   "metadata": {},
   "source": [
    "Since this is a classification problem, our first step is to check how imbalanced the classes are.\n",
    "\n",
    "**Task**: Measure the class imbalance in the dataset by calculating how often the target takes the value of interest (in this case, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5b497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc629e8",
   "metadata": {},
   "source": [
    "That means any model with accuracy below 90% is actually worse than a trivial model that always predicts \"not a pulsar.\"\n",
    "\n",
    "But the challenge doesn't stop at measuring performance—there's also a problem with how we split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "imbalance_rate_train = 100 * y_train.mean()\n",
    "print(f\"Interest class rate: {imbalance_rate_train:.2f}% (train)\")\n",
    "imbalance_rate_test = 100 * y_test.mean()\n",
    "print(f\"Interest class rate: {imbalance_rate_test:.2f}% (test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ba1e3",
   "metadata": {},
   "source": [
    "The [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function splits the dataset randomly, which means it doesn't preserve the class imbalance in each split.\n",
    "To fix this, we can use the *stratify* parameter.\n",
    "\n",
    "**Task** : Modify the code above to include the *stratify* parameter, then check the result and conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118ee48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "457c462d",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "As always, the bread and butter of data science : data preparation.\n",
    "\n",
    "**Task** : Inspect the dataset with the [`describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method as every column are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e2333b",
   "metadata": {},
   "source": [
    "**Task** : Given the code below, using [`scatter_matrix`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) function, interpret its output for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68314e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style(style=\"whitegrid\")\n",
    "\n",
    "X = df.drop(columns=[\"target\"])\n",
    "pd.plotting.scatter_matrix(X, figsize=(12, 6), alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0d6f7",
   "metadata": {},
   "source": [
    "This figure gives us plenty of insights, but it doesn't show the actual distribution of the two classes.\n",
    "\n",
    "**Task**: Using the function provided below, re-explore the relationships between the variables and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8748a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_classification(df, target_column, column_1, column_2, figsize=(12, 6), **kwargs):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for index, value in enumerate(df[target_column].unique()):\n",
    "        subset = df.loc[df[\"target\"] == value, ]\n",
    "        plt.scatter(subset[column_1], subset[column_2], color=sns.color_palette()[index], label=f\"Class {index}\", **kwargs)\n",
    "    plt.title(f\"{column_1} vs {column_2}\")\n",
    "    plt.xlabel(column_1)\n",
    "    plt.ylabel(column_2)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addbfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "900ba5ba",
   "metadata": {},
   "source": [
    "We clearly get that : Skewness = Excess_kurtosis ** 2. Therefore, we decide to remove the Excess_kurtosis variable from our work.\n",
    "Also, given the name of the column *Mean* and *Std*, we decide to compute the *Z-Score* variable defined as the ratio between the two.\n",
    "\n",
    "**Task** : Implement the changes described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159802f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd8936d",
   "metadata": {},
   "source": [
    "Now is time model !\n",
    "\n",
    "## Modelisation and pipeline\n",
    "\n",
    "Logistic regression learns its coefficient using variants of gradient descent. Therefore it requires a standardisation of its feature to stabilize the training.\n",
    "\n",
    "**Task** : Using the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) class, train a [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) after splitting with the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function carefully.\n",
    "Then, display performance metric such as accuracy, precision, recall and F1-Score. One can use the [`precision_recall_fscore_support`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7becd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9264671",
   "metadata": {},
   "source": [
    "The performance are already quite good, as expected ! Currently, we perform two steps when training:\n",
    "1. Learn and standardize the inputs\n",
    "2. Train the model\n",
    "\n",
    "And two steps when predicting:\n",
    "1. Standardize the input using the scaling learned from training\n",
    "2. Make predictions\n",
    "\n",
    "These two workflows are very similar, so we can combine them using a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) class. The goal is to simplify the code, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd379ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logistic\", LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_true=y_test, y_pred=y_pred, average=\"binary\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f010f",
   "metadata": {},
   "source": [
    "We highly encourage to use the `Pipeline` class to avoid data leakage when applying the `StandardScaler`.\n",
    "\n",
    "## Choosing the right threshold\n",
    "\n",
    "Logistic regression outputs a score that can be interpreted as a probability. By default, this score is converted into class labels (0 or 1) using a threshold of 0.5\n",
    "But what if 0.5 isn't the best threshold for optimizing the F1-score?\n",
    "\n",
    "**Task** : Compute for several thresholds the precision, recall and F1-Score value. Then make a plot highlighting where the best performance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6c4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df9dab00",
   "metadata": {},
   "source": [
    "We're going to dive deeper into some topics in the rest of this notebook, so let's first wrap up the useful code.\n",
    "\n",
    "**Task** : Define a function `train_experiment` which will fit a model given a training dataset, then test this model against a test dataset and display the metrics curves we've just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9603ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc36fae",
   "metadata": {},
   "source": [
    "## Handle imbalance\n",
    "\n",
    "So far, aside from splitting the data and measuring performance, class imbalance hasn't caused much trouble because the problem seems relatively simple.\n",
    "However, in general, imbalanced datasets often require resampling techniques. We'll cover some of these approaches.\n",
    "\n",
    "The first method is **random under-sampling**. This technique randomly removes observations from the majority class until the desired ratio between the majority and minority classes is reached.\n",
    "\n",
    "**Task** : Using the [`RandomUnderSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html) class, balance the training dataset and measure the number of row before and after. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec61120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053cfbf6",
   "metadata": {},
   "source": [
    "The second method works in the opposite direction: we duplicate observations from the minority class until we reach the desired ratio. This is called **random over-sampling**.\n",
    "\n",
    "**Task** : Using the [`RandomOverSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) class, balance the training dataset and measure the number of row before and after. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba931360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1366918f",
   "metadata": {},
   "source": [
    "Another method, also in the under-sampling family, is [**SMOTE**](https://www.jair.org/index.php/jair/article/view/10302/24590) (Synthetic Minority Over-sampling Technique). Instead of simply duplicating samples, SMOTE creates new observations by interpolating between existing minority-class examples and their neighbors. For a more visual explanation, one can look at [these slides](https://github.com/theo-lq/Conferences/blob/main/M2%20IASD%20Exec%20-%20ML%20LCLF/Support%202024.pdf) (in french).\n",
    "\n",
    "**Task** : Using the [`SMOTE`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) class, balance the training dataset and measure the number of row before and after. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ee91b7",
   "metadata": {},
   "source": [
    "For our dataset and setup, balancing the dataset didn't improve performance much, but it did affect the model's confidence and calibration.\n",
    "\n",
    "Another approach, which doesn't require changing the dataset, is to incorporate class imbalance directly into the training loss.\n",
    "\n",
    "**Task** : Using the *class_weight* parameter in the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class, fit a model and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1096909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b383ca",
   "metadata": {},
   "source": [
    "## Polynomial feature and hyperparameter tuning\n",
    "\n",
    "As we saw in Session 2, polynomial features can help linear models capture more complex relationships. It's worth trying them again here.\n",
    "\n",
    "**Task** : Using the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) class, process the training set and transform the test set. Then, fit as usual and display performance metrics and visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec38ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc283a68",
   "metadata": {},
   "source": [
    "It's a bit better, but there's still room for improvement. Now it's your turn to experiment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
