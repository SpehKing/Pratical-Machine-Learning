{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c06f0f0",
   "metadata": {},
   "source": [
    "## Lab Work 5: Fair Evaluation and Monte Carlo\n",
    "\n",
    "This notebook builds on the third lecture of Foundations of Machine Learning. We'll focus on Fair Evaluation and Monte Carlo.\n",
    "\n",
    "Important note: the steps shown here are not always the most efficient or the most \"industry-approved.\" Their main purpose is pedagogical. So don't panic if something looks suboptimal—it's meant to be.\n",
    "\n",
    "If you have questions (theoretical or practical), don't hesitate to bug your lecturer.\n",
    "\n",
    "First the necessary imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f05a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c380136",
   "metadata": {},
   "source": [
    "### 1. Load the dataset\n",
    "\n",
    "For this lecture we will use the Pulsar dataset you used in the previous Lecture.\n",
    "\n",
    "Taks: load it, print the head and split features from target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3007b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1886bf64",
   "metadata": {},
   "source": [
    "### 2. Stratified K-Fold Cross-Validation\n",
    "\n",
    "Goal: Split the dataset into k folds for cross-validation while preserving the proportion of classes in each fold (important for imbalanced datasets).\n",
    "\n",
    "Procedure: use sklearn to create the stratification, the pipeline and to iterate through the splits. Then print the summary statistic (balanced accuracy).\n",
    "\n",
    "### Accuracy vs Balanced Accuracy\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "- Measures the proportion of correctly classified samples:  \n",
    "   $\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$\n",
    "- Works well when classes are balanced.\n",
    "- Can be **misleading for imbalanced datasets**, because predicting the majority class gives high accuracy even if the model ignores the minority class.\n",
    "\n",
    "**Balanced Accuracy:**\n",
    "\n",
    "- Averages the recall for each class, giving equal weight to all classes:  \n",
    "   $\n",
    "\\text{Balanced Accuracy} = \\frac{1}{2} \\left( \\frac{\\text{TP}}{\\text{TP + FN}} + \\frac{\\text{TN}}{\\text{TN + FP}} \\right)\n",
    "$\n",
    "- Ensures the model performs well on **both majority and minority classes**.\n",
    "- Particularly useful for **imbalanced classification problems**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48ccc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1079d450",
   "metadata": {},
   "source": [
    "### 3. Bootstrap\n",
    "\n",
    "**Goal:** Estimate the variability of model performance and compute confidence intervals.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "- Resample the training data **with replacement** to create bootstrap datasets.\n",
    "- Use the **out-of-bag samples** as test sets.\n",
    "- Fit the pipeline on each bootstrap sample and record accuracy.\n",
    "- Repeat for `B` iterations and compute a **95% confidence interval** from the distribution of accuracies.\n",
    "- Visualize with a histogram and mark the mean accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f9156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0751f969",
   "metadata": {},
   "source": [
    "### 4. Permutation Test — Single Feature\n",
    "\n",
    "**Goal:** Assess the importance of a single feature for model performance.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "- Compute baseline accuracy of the model with all features.\n",
    "- Randomly **shuffle the values** of the feature of interest.\n",
    "- Evaluate the model on the permuted data.\n",
    "- Repeat for multiple permutations to get a distribution of accuracies.\n",
    "- Compare baseline accuracy to the permuted distribution: a large drop indicates a **relevant feature**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9f8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ab77ed",
   "metadata": {},
   "source": [
    "### 5. Permutation Importance — All Features\n",
    "\n",
    "**Goal:** Quantify the relevance of each feature for model performance.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "- Fit the pipeline on the full dataset.\n",
    "- Use `sklearn.inspection.permutation_importance` to shuffle each feature multiple times.\n",
    "- Measure the decrease in model accuracy for each feature.\n",
    "- Features causing a large drop are **more important**.\n",
    "- Visualize the results with a bar chart of mean decreases in accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0234bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f24488b0",
   "metadata": {},
   "source": [
    "### 6. Permutation Importance for prediction vs learning\n",
    "\n",
    "Goal: Notice the difference between predictive dependence (how much the model actually uses the feature) and learning dependence (how much the model needs the feature to achieve good performance)\n",
    "\n",
    "Task: create two groups of features, permute and **refit**. Compare the drop in accuracy: why is it different from the single feature permutation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02760dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03eaf26",
   "metadata": {},
   "source": [
    "### 7. Monte Carlo Permutation Test\n",
    "\n",
    "**Goal:** Test whether the model's performance is significantly better than chance.\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "- Randomly **permute the target labels** multiple times (B iterations).\n",
    "- Fit the model on each permuted dataset and record the accuracy.\n",
    "- Compare the true model accuracy to the distribution of accuracies under the null hypothesis.\n",
    "- Compute the **p-value**: proportion of permuted accuracies ≥ true accuracy.\n",
    "- Visualize with a histogram marking the true accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4960fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
